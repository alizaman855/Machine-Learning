{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lab2ex1\n",
    "\n",
    "import torch  # Import the PyTorch library\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Create a tensor\n",
    "print(\"Tensor:\", x)  # Print the tensor\n",
    "print(\"Tensor device:\", x.device)  # Print the device where the tensor is stored\n",
    "print(\"CUDA GPU:\", torch.cuda.is_available())  # Check if CUDA GPU is available\n",
    "if torch.cuda.is_available():  # Check if CUDA GPU is available\n",
    "    gpu_tensor = x.to('cuda')  # Move tensor to CUDA GPU\n",
    "    x = x.to(\"cuda\")  # Move tensor to CUDA GPU\n",
    "print(x)  # Print the tensor\n",
    "print(\"Tensor device:\", x.device)  # Print the device where the tensor is stored \n",
    "\n",
    "\n",
    "##Training a PyTorch model on the Mnist using CPU. Then, try with the GPU. ##ex2\n",
    "\n",
    "from __future__ import print_function  # Import print function from future module for compatibility with Python 2.x\n",
    "import argparse  # Import argparse module for parsing command-line arguments\n",
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network modules from PyTorch\n",
    "import torch.nn.functional as F  # Import functional interface to neural network modules from PyTorch\n",
    "import torch.optim as optim  # Import optimization algorithms from PyTorch\n",
    "from torchvision import datasets, transforms  # Import datasets and transforms from torchvision\n",
    "from torch.optim.lr_scheduler import StepLR  # Import learning rate scheduler from PyTorch\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # First convolutional layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # Second convolutional layer\n",
    "        self.dropout1 = nn.Dropout(0.25)  # First dropout layer\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Second dropout layer\n",
    "        self.fc1 = nn.Linear(9216, 128)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)  # Second fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # First convolutional operation\n",
    "        x = F.relu(x)  # ReLU activation\n",
    "        x = self.conv2(x)  # Second convolutional operation\n",
    "        x = F.relu(x)  # ReLU activation\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling operation\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for fully connected layers\n",
    "        x = self.fc1(x)  # First fully connected operation\n",
    "        x = F.relu(x)  # ReLU activation\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # Second fully connected operation\n",
    "        output = F.log_softmax(x, dim=1)  # Log softmax activation\n",
    "        return output\n",
    "\n",
    "# Define the training function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = F.nll_loss(output, target)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))  # Print training progress\n",
    "\n",
    "# Define the test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data to device\n",
    "            output = model(data)  # Forward pass\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))  # Print test results\n",
    "\n",
    "# Define the main function\n",
    "def main():\n",
    "    batch_size = 64  # Define batch size for training\n",
    "    test_batch_size = 1000  # Define batch size for testing\n",
    "    epochs = 14  # Define number of epochs\n",
    "    lr = 1.0  # Define learning rate\n",
    "    gamma = 0.7  # Define gamma for StepLR\n",
    "    no_cuda = False  # Flag for not using CUDA\n",
    "    no_mps = False  # Flag for not using MPS\n",
    "    dry_run = False  # Flag for dry run\n",
    "    seed = 1  # Define random seed\n",
    "    log_interval = 10  # Define log interval\n",
    "    save_model = False  # Flag for saving the model\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()  # Check if CUDA is available\n",
    "    use_mps = not no_mps and torch.backends.mps.is_available()  # Check if MPS is available\n",
    "    torch.manual_seed(seed)  # Set random seed\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")  # Set device to CUDA if available\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")  # Set device to MPS if available\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")  # Set device to CPU if neither CUDA nor MPS is available\n",
    "\n",
    "    # Define data loaders\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f5188-7bcb-4412-a047-71bfeeb94af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the gradient of a Tensor\n",
    "#Define a Tensor and compute its gradients using PyTorch.\n",
    "\n",
    "##lab3ex1\n",
    "import torch  # Import the PyTorch library\n",
    "\n",
    "# Define input, ground truth, weight, and bias tensors\n",
    "x = torch.tensor(2)  # Input tensor\n",
    "GT = torch.tensor(10)  # Ground truth tensor\n",
    "w = torch.tensor(3.0, requires_grad=True)  # Weight tensor with gradient tracking enabled\n",
    "b = torch.tensor(5.0, requires_grad=True)  # Bias tensor with gradient tracking enabled\n",
    "# Print tensors\n",
    "print(\"x:\", x)\n",
    "print(\"w:\", w)\n",
    "print(\"b:\", b)\n",
    "# Calculate the output\n",
    "y = w * x + b\n",
    "# Print the output\n",
    "print(\"y:\", y)\n",
    "# Calculate the loss\n",
    "loss = (y - GT) ** 2\n",
    "# Backpropagation: Calculate gradients\n",
    "loss.backward()\n",
    "# Get gradients\n",
    "dx = x.grad\n",
    "dw = w.grad\n",
    "db = b.grad\n",
    "# Print gradients\n",
    "print(\"x.grad :\", dx)\n",
    "print(\"w.grad :\", dw)\n",
    "print(\"b.grad :\", db)\n",
    "\n",
    "#BackPropagation Create the simple Network shown bellow and apply BackPropagation to compute the gradient of the loss with respect to the weight>\n",
    "\n",
    "##ex2\n",
    "import torch  # Import the PyTorch library\n",
    "# Define input, output, and weight tensors\n",
    "x = torch.tensor(2)  # Input tensor\n",
    "y = torch.tensor(10)  # Output tensor\n",
    "w = torch.tensor(3.0, requires_grad=True)  # Weight tensor with gradient tracking enabled\n",
    "# Print tensors\n",
    "print(\"x:\", x)\n",
    "print(\"w:\", w)\n",
    "# Calculate the predicted output\n",
    "y_hat = w * x\n",
    "# Print the predicted output\n",
    "print(\"y_hat:\", y_hat)\n",
    "# Calculate the error\n",
    "s = y_hat - y\n",
    "# Calculate the loss\n",
    "loss = s ** 2\n",
    "# Backpropagation: Calculate gradients\n",
    "loss.backward()\n",
    "# Get gradients\n",
    "dx = x.grad\n",
    "dw = w.grad\n",
    "# Print gradients\n",
    "print(\"x.grad :\", dx)\n",
    "print(\"w.grad :\", dw)\n",
    "\n",
    "\n",
    "##ex3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "def show(model, test_loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    output1 = model(images)\n",
    "    pred1 = output1.argmax(dim=1, keepdim=True)\n",
    "    for i in range(9):\n",
    "        print('GT # ', i ,' : ', labels[i])\n",
    "        print('Prediction # ', i , ' : ', pred1[i],'\\n')\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(images[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.show()\n",
    "def main():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 1\n",
    "    lr = 1.0\n",
    "    gamma = 0.7\n",
    "    log_interval = 10\n",
    "    save_model = True\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset1 = datasets.MNIST(r\"C:\\Users\\wesam\\data\\MNIST\", train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.MNIST(r\"C:\\Users\\wesam\\data\\MNIST\", train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, batch_size=test_batch_size)\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "        test(model, device, test_loader)\n",
    "        show(model, test_loader)\n",
    "        scheduler.step()\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "## Q4 Optimize the Mnist model for best performance with the CIFAR-10 dataset:\n",
    "##ex4\n",
    "import torch\n",
    "xx = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "print('xx:', xx)\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "y1 = x + 1\n",
    "print(\"y1:\", y1)\n",
    "y2 = x**2\n",
    "print(\"y2:\", y2)\n",
    "y3 = x*2 + x**2\n",
    "print(\"y3:\", y3)\n",
    "y3.backward()\n",
    "dx = x.grad\n",
    "print(\"x.grad:\", dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write the training and testing functions from scratch for the Mnist dataset without using any parameter.\n",
    "##lab4 ex1\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "def train( model):\n",
    "    train_kwargs = {'batch_size': 64}\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    #CIFAR10\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(torch.device(\"cpu\")), target.to(torch.device(\"cpu\"))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "def test(model):\n",
    "    test_kwargs = {'batch_size': 1000}\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        #CIFAR10\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs) \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(torch.device(\"cpu\")), target.to(torch.device(\"cpu\"))      \n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "def show_(model,  test_loader):\n",
    "    test_kwargs = {'batch_size': 1000}\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "             #CIFAR10\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs) \n",
    "    model.eval()   \n",
    "    images, labels = next(iter(test_loader))\n",
    "    output1 = model(images)\n",
    "    pred1 = output1.argmax(dim=1, keepdim=True)   \n",
    "    for i in range(9):\n",
    "        print('GT # ', i ,' : ', labels[i])\n",
    "        print('Predition # ', i , ' : ', pred1[i],'\\n')\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(images[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.show()   \n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    model = Net().to(torch.device(\"cpu\"))\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train( model)\n",
    "        test(model)\n",
    "        show_(model)\n",
    "        scheduler.step()\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "##Study the definition of ResNet (you can check the references bellow). Then, Run a ResNet 10 code for the CIFAR10 dataset. \n",
    "#ex2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lab5 ex1: Update the MNIST code of the previous lectures such that it uses RNN based model. As each image is 28 pixels (rows) by 28 pixels (cols). \n",
    "rom __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=28,\n",
    "            hidden_size=128,         # rnn hidden unit - CHANGE THE NUMBER OF HIDDEN LAYER\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        r_out, _ = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(-1, 28, 28)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 28, 28)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "def show_(model,  test_loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    data =  images\n",
    "    data = data.view(-1, 28, 28)    \n",
    "    output1 = model(data)\n",
    "    pred1 = output1.argmax(dim=1, keepdim=True) \n",
    "    for i in range(9):\n",
    "        print('GT # ', i ,' : ', labels[i])\n",
    "        print('Predition # ', i , ' : ', pred1[i],'\\n')\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(images[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.show() \n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        show_(model,  test_loader)\n",
    "        scheduler.step()\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lab6 : Update your RNN code which you develop in lab 5 to use LSTM (Long short-term memory) layer instead of the RNN layer for MNIST classification\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(         # if use nn.LSTM(), it hardly learns\n",
    "            input_size=28,\n",
    "            hidden_size=128,         # lstm hidden unit\n",
    "            num_layers=3,           # number of lstm layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        r_out, _ = self.lstm(x, None)   # None represents zero initial hidden state\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(-1, 28, 28)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "def show_(model,  test_loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    data =  images\n",
    "    data = data.view(-1, 28, 28)    \n",
    "    output1 = model(data)\n",
    "    pred1 = output1.argmax(dim=1, keepdim=True)\n",
    "    for i in range(9):\n",
    "        print('GT # ', i ,' : ', labels[i])\n",
    "        print('Predition # ', i , ' : ', pred1[i],'\\n')\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(images[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.show()  \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                    help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                    help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                    help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                    help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                    help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                    help='For Saving the current Model')\n",
    "# Add a catch-all argument to capture any additional arguments\n",
    "    parser.add_argument('unrecognized', nargs='*')\n",
    "    args, unrecognized_args = parser.parse_known_args()\n",
    "# Check if there are any unrecognized arguments and print a warning if so\n",
    "    if unrecognized_args:\n",
    "        print(f\"Warning: Unrecognized arguments: {unrecognized_args}\")\n",
    "    torch.manual_seed(args.seed)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    #CIFAR10'\n",
    "    dataset1 = datasets.MNIST(r\"C:\\Users\\wesam\\data\\MNIST\", train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST(r\"C:\\Users\\wesam\\data\\MNIST\", train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        show_(model,  test_loader)\n",
    "        scheduler.step()\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "##ex Update your LSTM code of Exercise 1 to use GRU (gated recurrent unit) layer instead of the LSTM layer for MNIST classification\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gru = nn.GRU(         # if use nn.LSTM(), it hardly learns\n",
    "            input_size=28,\n",
    "            hidden_size=128,         # lstm hidden unit\n",
    "            num_layers=3,           # number of lstm layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        r_out, _ = self.gru(x, None)   # None represents zero initial hidden state\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(-1, 28, 28) \n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    i=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "def show_(model,  test_loader):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    data =  images\n",
    "    data = data.view(-1, 28, 28)    \n",
    "    output1 = model(data)\n",
    "    pred1 = output1.argmax(dim=1, keepdim=True)\n",
    "    for i in range(9):\n",
    "        print('GT # ', i ,' : ', labels[i])\n",
    "        print('Predition # ', i , ' : ', pred1[i],'\\n')\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(images[i].reshape(28,28), cmap=\"gray\")\n",
    "    plt.show() \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                    help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                    help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                    help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                    help='disables macOS GPU training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                    help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                    help='For Saving the current Model')\n",
    "# Add a catch-all argument to capture any additional arguments\n",
    "    parser.add_argument('unrecognized', nargs='*')\n",
    "    args, unrecognized_args = parser.parse_known_args()\n",
    "# Check if there are any unrecognized arguments and print a warning if so\n",
    "    if unrecognized_args:\n",
    "        print(f\"Warning: Unrecognized arguments: {unrecognized_args}\")\n",
    "    torch.manual_seed(args.seed)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    #CIFAR10\n",
    "    dataset1 = datasets.MNIST(r\"C:\\Users\\wesam\\data\\MNIST\", train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST(r\"C:\\Users\\wesam\\data\\MNIST\", train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        show_(model,  test_loader)\n",
    "        scheduler.step()\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##lab7\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "class Transformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)     \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt) \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)    \n",
    "        return out   \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0     \n",
    "        return mask \n",
    "#     def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "#         # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "#         # [False, False, False, True, True, True]\n",
    "#         return (matrix == pad_token)\n",
    "    def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "    data = []\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "        X[start::2] = 1\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "        data.append([X, y])\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "   def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "    return batches\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(\n",
    "    num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "    def train_loop(model, opt, loss_fn, dataloader):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:] \n",
    "        # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "        # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.detach().item()\n",
    "            return total_loss / len(dataloader)\n",
    "        def main(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25) \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print() \n",
    "    return train_loss_list, validation_loss_list\n",
    "train_loss_list, validation_loss_list = main(model, opt, loss_fn, train_dataloader, val_dataloader, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lab8 Use PyTorch Geometric (PyG) python library to model the graph neural network for paper classification task on the CORA dataset\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.logging import init_wandb, log\n",
    "from torch_geometric.nn import GCNConv\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='Cora')\n",
    "parser.add_argument('--hidden_channels', type=int, default=16)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--epochs', type=int, default=200)\n",
    "parser.add_argument('--use_gdc', action='store_true', help='Use GDC')\n",
    "parser.add_argument('--wandb', action='store_true', help='Track experiment')\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "init_wandb(name=f'GCN-{args.dataset}', lr=args.lr, epochs=args.epochs,\n",
    "           hidden_channels=args.hidden_channels, device=device)\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, args.dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "if args.use_gdc:\n",
    "    transform = T.GDC(\n",
    "        self_loop_weight=1,\n",
    "        normalization_in='sym',\n",
    "        normalization_out='col',\n",
    "        diffusion_kwargs=dict(method='ppr', alpha=0.05),\n",
    "        sparsification_kwargs=dict(method='topk', k=128, dim=0),\n",
    "        exact=True,\n",
    "    )\n",
    "    data = transform(data)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n",
    "                             normalize=not args.use_gdc)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n",
    "                             normalize=not args.use_gdc)\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "model = GCN(dataset.num_features, args.hidden_channels, dataset.num_classes)\n",
    "model, data = model.to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "], lr=args.lr)  # Only perform weight-decay on first convolution.\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr).argmax(dim=-1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "best_val_acc = final_test_acc = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log(Epoch=epoch, Loss=loss, Train=train_acc, Val=val_acc, Test=test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d482bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lab9 Write a PyTorch code to implement a VAE in order to generate digits images similar to the MNIST dataset.\n",
    "# Model Hyperparameters\n",
    "dataset_path = '~/datasets'\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "batch_size = 100\n",
    "x_dim  = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "lr = 1e-3\n",
    "epochs = 30\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)\n",
    "lass Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)   \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)    \n",
    "        self.training = True     \n",
    "    def forward(self, x):\n",
    "        h_       = self.LeakyReLU(self.FC_input(x))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance                                                       #             (i.e., parateters of simple tractable normal distribution \"q\"    \n",
    "        return mean, log_var\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        \n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder    \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z              \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat            = self.Decoder(z)       \n",
    "        return x_hat, mean, log_var\n",
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)\n",
    "from torch.optim import Adam\n",
    "BCE_loss = nn.BCELoss()\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "print(\"Start training VAE...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(x, x_hat, mean, log_var)\n",
    "        \n",
    "        overall_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n",
    "    \n",
    "print(\"Finish!!\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)    \n",
    "        x_hat, _, _ = model(x)\n",
    "        break\n",
    "def show_image(x, idx):\n",
    "    x = x.view(batch_size, 28, 28)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(x[idx].cpu().numpy())\n",
    "show_image(x, idx=0)\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(batch_size, latent_dim).to(DEVICE)\n",
    "    generated_images = decoder(noise)\n",
    "save_image(generated_images.view(batch_size, 1, 28, 28), 'generated_sample.png')\n",
    "show_image(generated_images, idx=12)\n",
    "show_image(generated_images, idx=0)\n",
    "show_image(generated_images, idx=1)\n",
    "show_image(generated_images, idx=10)\n",
    "show_image(generated_images, idx=20)\n",
    "show_image(generated_images, idx=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
